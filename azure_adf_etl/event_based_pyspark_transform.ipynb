{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee6b04d-e0fd-4fae-ad61-899f1de5f8c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the return values from the parameters of Azure Data Factory\n",
    "file_name = dbutils.widgets.get('fileName')\n",
    "file_path = dbutils.widgets.get('filePath')\n",
    "\n",
    "print(file_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46a002e3-0ca2-4e27-8410-1d4d962717c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define mount configs\n",
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
    "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(<your_token_provider_class_name_here>)\n",
    "}\n",
    "\n",
    "# Mount the ADLS Container\n",
    "dbutils.fs.mount(\n",
    "  source = \"abfss://movies@armieldatalake.dfs.core.windows.net/\",\n",
    "  mount_point = \"/mnt/movies/\",\n",
    "  extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ff1b1b-7357-4b6f-85c0-ef18388c2323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the created CSV File\n",
    "df = spark.read.csv(\n",
    "    f'/mnt/{file_path}/{file_name}',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364eba92-26aa-473e-aeb4-146814d181dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, first, max, format_number, count\n",
    "\n",
    "# Remove invalid row values from the column `Budget (in $)` and `Distributor`, rename column `_c0` to `movie_id`\n",
    "df_cleanup = df.filter((col('Budget (in $)').rlike('^[0-9]+$')) & (col('Distributor').rlike('^[^0-9]*$'))).withColumnRenamed('_c0', 'movie_id')\n",
    "\n",
    "# Apply aggregate functions\n",
    "grouped_df = df_cleanup.groupby('distributor').agg(\n",
    "    format_number(mean('World Wide Sales (in $)'), 0).alias('avg_sale_per_distributor'),\n",
    "    count('movie_id').alias('distributor_n_movies'),\n",
    "    first('Title').alias('top_movie'),\n",
    "    format_number(max('World Wide Sales (in $)').cast('int'), 0).alias('top_movie_sales')\n",
    "    )\n",
    "\n",
    "# Get the top 10 distributor in terms of sales\n",
    "result = grouped_df.orderBy('avg_sale_per_distributor', ascending=False).limit(10)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ba1cb6-d250-49ac-b72f-2fa1dbb64ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the container output path\n",
    "output_path = '/mnt/movies/transformed/'\n",
    "\n",
    "# Save the dataframe in parquet format\n",
    "result.write.mode('overwrite').csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbea1372-3754-4ef3-947d-a9fb1a343a80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Azure SQL Database connection parameters\n",
    "jdbc_url = dbutils.secrets.get(scope='azure-sql-secret', key='jdbc-url')\n",
    "user = dbutils.secrets.get(scope='azure-sql-secret', key='jdbc-user')\n",
    "password = dbutils.secrets.get(scope='azure-sql-secret', key='jdbc-password')\n",
    "\n",
    "# Define the table name to save the DataFrame\n",
    "table_name = \"top_grossing_movies\"\n",
    "\n",
    "# Save the DataFrame to Azure SQL Database\n",
    "result.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b3cfc12-1e8a-467a-8d8a-731a28984c5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Azure Synapse SQL Database connection parameters\n",
    "jdbc_url = dbutils.secrets.get(scope='azure-sql-secret', key='dw-jdbc-url')\n",
    "user = dbutils.secrets.get(scope='azure-sql-secret', key='dw-jdbc-user')\n",
    "password = dbutils.secrets.get(scope='azure-sql-secret', key='dw-jdbc-password')\n",
    "\n",
    "# Define the table name to save the DataFrame\n",
    "table_name = \"top_grossing_movies\"\n",
    "\n",
    "# Save the DataFrame to Azure Synapse SQL pool\n",
    "result.write \\\n",
    "  .format(\"com.databricks.spark.sqldw\") \\\n",
    "  .option(\"url\", jdbc_url) \\\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n",
    "  .option(\"dbtable\", table_name) \\\n",
    "  .option(\"user\", user) \\\n",
    "  .option(\"password\", password) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57163722-2253-4eac-b299-bda3776739e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unmount the ADLS\n",
    "dbutils.fs.unmount('/mnt/movies/')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "event_based_pyspark_transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
